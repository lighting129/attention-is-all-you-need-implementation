Transformer Model Implementation

This repository contains an implementation of a Transformer model using PyTorch. The model is built from scratch, covering essential components like self-attention, multi-head attention, encoder-decoder layers, and positional encoding. This project can serve as a base for various NLP tasks such as machine translation, text summarization, and more.

Overview

This project implements the Transformer architecture as introduced in the paper “Attention Is All You Need”. The Transformer model revolutionized NLP tasks with its attention mechanisms and parallelization advantages. This repository aims to help users understand and experiment with the core components of a Transformer model.

Features

	•	Custom Self-Attention Layer: Implements scaled dot-product self-attention.
	•	Multi-Head Attention: Divides the attention mechanism into multiple heads for improved learning.
	•	Positional Encoding: Adds positional embeddings to retain sequential information.
	•	Encoder and Decoder Blocks: Built from scratch, with normalization, residual connections, and dropout for regularization.
	•	Configurable Parameters: Easily adjustable parameters like embedding size, number of heads, and dropout rate.


Credits

Special thanks to Aladdin Persson’s tutorial on building a Transformer model from scratch. His video provided valuable guidance and insights that helped shape this implementation.
